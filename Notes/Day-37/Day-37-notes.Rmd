# Day 37 Notes: Dec. 2, 2013

```{r include=FALSE}
require(mosaic, quietly=TRUE)
opts_chunk$set(tidy=FALSE,dev="svg",out.width="80%")
```
## Review: Traditional Tests

* p test
* t test
* one-way anova
* simple regression

These miss out on the ability to include covariates.  They do not give a measure of effect size.

One that we have not covered:

* $\chi^2$ (chi-squared) tests.

### The $\chi^2$ test ("chi squared")

Chi-squared is a much beloved test, one of the first statistical tests.  There's also a chi-squared distribution, which is analogous to an F distribution or a t distribution and has, as a parameter, "degrees of freedom".  It is associated with Karl Pearson the dean of statistics at the turn of the 19th-20th century.

The $\chi^2$ test is used for two distinct but related purposes: 
* Testing for "independence" of two variables.  Our analogous term is "interaction".
* Comparing an observed set of frequencies to a model probability.

Example: In genetics, [di-hybrid cross](http://www.biology.arizona.edu/mendelian_genetics/problem_sets/dihybrid_cross/03t.html) involves mating two organisms, one with a dominant trait and one with a recessive trait.  Their offspring are then mated, producing 4 phenotypes in the second generation.  The second generation has, theoretically, a rate of expression of 9:3:3:1.  For instance, in an agricultural experiment, 1301 plants have been grown from a dihybrid cross and the four phenotypes appear like this:

Phenotype | Observed | Expected in Theory
----------|----------| ------
Green   | 773      | 731.9
Golden   | 231       | 243.9
Green-striped | 238  | 243.9
Golden-green-striped     | 59      | 81.3
TOTAL     | 1301     | 1301

It's pretty obvious which phenotype corresponds to the "9" and which to the "1".  The question is whether the observation is consistent with the theory.

This example is drawn from Snedecor and Cochran (1989), *Statistical Methods* 8th ed.  p 196.  It draws on work done by Lindstrom (1918).  That the example dates from 1918 and is still being used in 1989 says something about the pedigree of the $\chi^2$ test.  Here are the data in a simple R form

```{r}
LindstromObs = c(773, 231, 238, 59)
LindstromExp = 1301*c(9,3,3,1)/16
```

In this class, we've focussed on fitting models to observed data with the goal of relating explanatory variables to a response.  In this dihybrid problem, however, the emphasis is different: we have a model and we want to know if the data are consistent with it.

We've already seen such questions in the context of the Null Hypothesis.  So let's let the theory be our null hypothesis and ask if the data are consistent with the null.  To do this, we need
1. a test statistic
2. the distribution of the test statistic under the Null.

An obvious test statistic to use is the sum of square residuals.  Here's a program to calculate that:
```{r}
ssResids = function(observed, expected) 
  sum((observed-expected)^2)
```

Our test statistic:
```{r}
ts = ssResids(observed=LindstromObs,expected=LindstromExp)
ts
```

To carry out a hypothesis test, we need to be able to generate samples from the Null Hypothesis.  Since we know the rate at which each phenotype is being generated, we can use the poisson random number generator.
```{r}
counts = rpois(4, lambda=LindstromExp )
counts
```

This method is good, but it's not necessarily consistent with the total number of observations, 1301.  So, a trick:
```{r}
makeSample = function(expected) {
  n = sum(expected)
  table( resample(1:length(expected), prob=expected/n, size=round(n)))
}
```

Now, the sampling distribution under the Null Hypothesis:
```{r}
s = do(1000)*ssResids(observed=makeSample(LindstromExp),expected=LindstromExp)
```

And the p-value:
```{r}
mean( result > ts, data=s)
```
The observed data are a little bit surprising.

As it happens, the test statistic that we developed historically is a bit different:
```{r}
chisq.stat = function(observed, expected){
  sum( (observed-expected)^2 / expected )
}
```

Running the hypothesis test. Here, the sampling distribution under the null:
```{r}
s2 = do(1000)*chisq.stat(observed=makeSample(LindstromExp), expected=LindstromExp)
t2 = chisq.stat(observed=LindstromObs,expected=LindstromExp)
mean( result > t2, data=s2 )
```
The p-value is lower than before, in fact below 0.05.  (Remember, there's nothing magical about 0.05.  It's just a convention.)  Quoting Snedecor: "Lindstrom commented that the deviations [from Mendelian genetic theory] could be largely explained by a physiological cause, namely the weakened condition of the last three classes due to their chlorophyll abnormality.  He pointed out in particular that the last class (golden-green-striped) was not very vigorous."

The change in the p-value occurred because the division by `expected` in the $\chi^2$ statistic means that equal weight is being put on each of the 4 categories.  In the simple sum of squares, equal weight is put on each of the cases.  

There's a reason to prefer the $\chi^2$ statistic to the simple sum of square residuals.  The reason is that we actually know what will be the variance of a poisson random variable.

ACTIVITY:
Generate a large poisson sample of a given $\lambda$. Find the standard deviation. Try this for several different $\lambda$, say $\lambda=25,100,400,1600,6400$.   
* How is the standard deviation related to $\lambda$?
* How is the variance related to $\lambda$?

By dividing (observed-expected)^2 by expected, we are effectively creating a z^2 variable, the square of a normal random variable with mean=0 and sd=1.  

There was some controversy in the early days about what the distribution of the sum of $k$ z^2 variables should be.  It was called the $\chi^2$ distribution, but it was observed that adding up three z^2 variables was closer to what was observed for 4 categories in the chi-squared test than adding up four z^2 variables.

Of course, making such a finding required that there be some definition of "closer to".  This involved some argument.  Remember, in 1900, calculations were difficult.  But now it's easy to see that three is better than four.  For example, when the distributions match, plotting one versus another (in sorted order) should give a straight line along the diagonal.
```{r}
z3 = do(1000)*sum( rnorm(3)^2 )
z4 = do(1000)*sum( rnorm(4)^2 )
plotPoints( sort(z3$result)~ sort(s2$result))
plotFun( x ~ x, add=TRUE, col="red")
plotPoints( sort(z4$result)~ sort(s2$result))
plotFun( x ~ x, add=TRUE, col="red")
mean(result, data=z3)
mean(result, data=z4)
mean(result, data=s2)
```
What's going on here is that the four observations had to add up to 515.  In other words, the residuals must add up to zero.  This is very much what happens when we fit a model: the residuals must add up to zero because we've fitted the model.  In this case, we've fitted the expectation so that it must add up to 515.  This requirement takes one degree of freedom out of the expected number.  Historically, thus was the concept of degrees of freedom born.

In the $\chi^2$ test you can see the origins of many of the concepts that have shaped our approach to statistical modeling: residuals, sum of square residuals as a figure of merit, the sampling distribution under a null hypothesis, degrees of freedom.


### The $\chi^2$ test for independence


An example from Snedecor (1989, p. 202) looks at 196 patients classified according to change in health of leprosy patients and degree of skin damage (called "infiltration").  

Degree of Infiltration | Marked Improvement | Moderate | Slight | Stationary | Worse | TOTAL
-----------------------|--------------------|----------|--------|------------|-------|-------
Little | 11 | 27 | 42 | 53 |  11 | 144
Much | 7 | 15 | 16 | 13 | 1 | 52
TOTAL | 18 | 42 | 58 | 66 | 12 | 196 


In our modeling framework, we would likely see whether there is a relationship between "Degree of Infiltration" and Improvement by making a logistic regression model with "Degree of Infiltration" as the response. The $\chi^2$ test constructs a null hypothesis probability model by assuming that the two variables are independent and then tests the chi-square statistic against that model.  Not much different.

Absent a modeling technique for multi-level response variables, we don't have an exact analog of the chi-squared test using modeling.

Note that the appropriate test here is actually "Fisher's Exact Test", which avoids an approximation made in the $\chi^2$ test.

#### Models of Counts

Here's a simulation example with a continuous predictor from [UCLA](http://www.ats.ucla.edu/stat/r/dae/poissonreg.htm).  The response is the number of awards given to students.  The explanatory variables are the score on a math exam and the type of program in which the student was enrolled.  The case is an individual student.

```{r}
p <- read.csv("http://www.ats.ucla.edu/stat/data/poisson_sim.csv")
p <- within(p, {
    prog <- factor(prog, levels = 1:3, labels = c("General", "Academic", "Vocational"))
    id <- factor(id)
})
summary(p)
```

And the analysis:
```{r}
m1 = glm(num_awards ~ prog + math, family = "poisson", data = p)
summary(m1)
```
```{r}
m2 = glm(num_awards ~ prog * math, family = "poisson", data = p)
summary(m2)
```

No sign of an interaction.




### Logistic Regression

When we do linear regression on a Zero/One variable, we are effectively modeling the probability.
* Easy to see if there is a single, categorical explanatory variable --- the model value is just the groupwise means in the different levels of the variable.
* A bit more subtle if the explanatory variable is quantitative.  Draw the picture with Zero/One response variable on the vertical axis and a quantitative variable on the horizontal axis.  Example: O-ring damage in the space shuttle.
```{r}
orings = fetchData("oring-damage.csv")
xyplot( damage ~ temp, data=orings,pch=20 )
```
Sketch in the best fitting line.  But a line is too "stiff" and needs eventually to escape 0-1.  

Remember that the idea of logistic regression is to fit a linear model to the "log odds", which is just the logarithm of $p/(1-p)$ --- a different format for the probability.  We haven't talked about **how** this is done.  That's more advanced.

### Odds and the Odds Ratio

Suppose we have some data on an illness and exposure to a potential toxin

            |Sick|Healthy
------------|---|---
Exposed     | A | B
Not Exposed | C | D

Among the exposed, the risk of being sick is $A/(A+B)$.

Among the unexposed, the risk of being sick is $C/(C+D)$.

The risk ratio is the ... ratio of the two risks!  It tells how much more likely you are to get the sickness if you are exposed.

Now suppose that the sickness is fairly rare, say roughly 1 in 1000.  It's a huge amount of work to measure the exposure on everybody.  Is it necessary, given that almost everybody is healthy.

To avoid this problem, it's common to do a **case/control** study, where we pick sick people from a clinic and a similar number of healthy controls.  (Call up the friends of the sick kids.  They will be similar in age, activities, etc.)

But now the ratio of A to B is wrong, it's roughly 1 to 1, whereas in the population it's roughly 1 to 1000

Instead, we calculate the odds and the odds ratio.  $\frac{A/C}{B/D}$  
Notice that if A is much less than B, and C is much less than D, the odds ratio is essentially the same as the risk ratio.


### The odds ratio in use

A study of bicycle helmet use and the influence of state law.

Demonstration of logistic regression and odds ratios.  "Effects of state helmet laws on bicycle helmet use by children and adolescents" [*Injury Prevention* **2002, 9**:42-46](http**://injuryprevention.bmj.com/content/8/1/42.full.pdf)

The coefficients in logistic regression correspond to log-odds ratios.

When they say "adjusted Odds Ratio", they mean the odds ratio of one of the categorical variable levels relative to the reference level, with the other variables in the model as covariates.  Notice that they give a confidence interval on the odds ratio itself.  The calculation of this is straightforward given the standard error on the logistic regression coefficient.

Calculate the confidence interval on the odds ratio, e.g.
```{r}
exp( .7 + c(-1,1)*1.96*.294)
```

### In the News
* [Predicting Judges' Rulings](http://www.nytimes.com/2012/11/27/us/judges-rulings-follow-partisan-lines.html?hp&_r=0)

## Introduction to Causal Reasoning

One way to define a causal relationship between A and B is this: If you change A, then B will change.

The accepted method to establish a causal relationship is to do an experiment: Change A and observe whether B changes correspondingly.  The key word here is "change."  Simply observing that A varies and B varies with it does not establish causation; it might equally have been that B causes A or that A and B both have a common cause.  You need to impose the change directly on A.

But, what to do when you can't do an experiment.

Recidivism and Parole.

Public health issues: not every smoker gets lung cancer, not every person with lung cancer was a smoker.

### Austin Bradford Hill

A famous article, from 1965, about [causation in public health](https://dl.dropboxusercontent.com/u/5098197/Epidemiology/Hill_1965.pdf).  It lays out 9 criteria to support a claim of a causal relationship, e.g. strength, consistency, temporality, specificity, plausibility, experiment.  

Hill, who followed Fisher as president of the Royal Statistical Society, dismisses *Tests of Significance*.

> No formal tests of significance can answer those questions [of causation].  Such tests can, and should, remind us of the effects that the play of chance can create, and they will instruct us in the likely magnitude of those effects.  Beyond that they contribute nothing to the 'proof' of our hypothesis.


#### Myopia

* [Myopia article from Nature](http://dl.dropbox.com/u/5098197/ISM/myopia.pdf).  Note the extensive scientific jargon and the dismissing of the lack of causal evidence: "Although it does not establish a causal link, the statistical strength of the association of night-time light exposure and child- hood myopia does suggest that the absence of a daily period of darkness during early childhood is a potential precipitating factor in the development of myopia."

It turns out that the parent's myopia is the disposing factor.  Shortsighted parents leave the lights on at night and their kids are shortsighted for genetic reasons.  <!-- A CNN news report is [here](http://www.cnn.com/HEALTH/9905/12/children.lights/index.htm). --> Here is a [rebuttal](http://www.nature.com/nature/journal/v404/n6774/full/404143a0.html).

